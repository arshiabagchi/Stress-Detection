{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "LOAD MODELS AND DEPENDENCIES"
      ],
      "metadata": {
        "id": "48QQUDtUg5h_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tLoJNvF3Cc-d",
        "outputId": "fbc6560d-61a1-4796-b38c-2bc7d699ec71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "#Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "67KmZhABCzAY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2407ecee-aa34-437c-986b-474aae0302b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['C4_values.npy', 'C6_values.npy', 'C5_values.npy', 'C3_values.npy', 'C2_values.npy', 'facial_model.h5', 'survey_model.h5', 'audio_model.h5', 'survey_scaler.pkl', 'survey_encoder.pkl']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# List files in your model directory\n",
        "print(os.listdir('/content/drive/MyDrive/saved_models'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "h1FizyFLQS24",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f107133-8d1e-46ea-8960-a816f1cfedde"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ],
      "source": [
        "#Loads pre-trained models from model directory\n",
        "import tensorflow as tf\n",
        "facial_model = tf.keras.models.load_model(\"/content/drive/MyDrive/saved_models/facial_model.h5\") #Facial Prediction\n",
        "audio_model = tf.keras.models.load_model(\"/content/drive/MyDrive/saved_models/audio_model.h5\") #Audio Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOoxThbKDLNn"
      },
      "source": [
        "FACIAL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "2ufuRLv_C9mf"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "#Function that takes an image path and loads model\n",
        "def predict_facial(photo_path, model):\n",
        "\n",
        "    \"\"\"\n",
        "    photo_path: path to the photo file\n",
        "    model: loaded keras facial model\n",
        "\n",
        "    Returns:\n",
        "        label: 'Stressed' or 'Not Stressed'\n",
        "        confidence: float, confidence for the predicted label\n",
        "    \"\"\"\n",
        "\n",
        "    # Load and preprocess image\n",
        "    #Reads the image from photo path, converts it to grayscale, resized and normalizes it\n",
        "    img = cv2.imread(photo_path)\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    resized = cv2.resize(gray, (48, 48))\n",
        "    normalized = resized / 255.0\n",
        "    input_img = normalized.reshape(1, 48, 48, 1)\n",
        "\n",
        "    # Predict\n",
        "    prediction = model.predict(input_img, verbose=0)\n",
        "    class_id = np.argmax(prediction)\n",
        "    confidence = float(prediction[0][class_id])\n",
        "    label = \"Stressed\" if class_id == 1 else \"Not Stressed\"\n",
        "    return label, confidence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cse_FC0QGoPe"
      },
      "source": [
        "SURVEY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "S89tO7YJYakO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ec87898-0757-47da-daee-adcf85da2bba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import joblib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Load your trained model\n",
        "model = tf.keras.models.load_model('/content/drive/MyDrive/saved_models/survey_model.h5')\n",
        "\n",
        "# Load the fitted encoder and scaler\n",
        "encoder = joblib.load('/content/drive/MyDrive/saved_models/survey_encoder.pkl')\n",
        "scaler = joblib.load('/content/drive/MyDrive/saved_models/survey_scaler.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "lLyOJdyTYvIP"
      },
      "outputs": [],
      "source": [
        "# Load unique values for each question (for Gradio radio buttons)\n",
        "features = ['C2', 'C3', 'C4', 'C5', 'C6']\n",
        "choices = []\n",
        "for col in features:\n",
        "    choices.append(sorted(np.load(f\"/content/drive/MyDrive/saved_models/{col}_values.npy\", allow_pickle=True)))\n",
        "\n",
        "#Function to predict stressed/not stressed based on survey answered by the user.\n",
        "def predict_survey(activity, companion, pressure, tired, energetic):\n",
        "    user_input = pd.DataFrame([{\n",
        "        'C2': activity,\n",
        "        'C3': companion,\n",
        "        'C4': pressure,\n",
        "        'C5': tired,\n",
        "        'C6': energetic\n",
        "    }])\n",
        "    encoded = encoder.transform(user_input[features])\n",
        "    scaled = scaler.transform(encoded)\n",
        "    prediction = model.predict(scaled)[0][0]\n",
        "    label = \"Stressed\" if prediction >= 0.5 else \"Not Stressed\"\n",
        "    confidence = round(float(prediction if prediction >= 0.5 else 1 - prediction), 2)\n",
        "    return label, confidence\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLgQW3RWITON"
      },
      "source": [
        "AUDIO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "XGrsMCZQSYLh"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import librosa\n",
        "\n",
        "#Audio model already loaded globally but can be loaded again.\n",
        "#audio_model = tf.keras.models.load_model('/content/drive/MyDrive/saved_models/audio_model.h5')\n",
        "\n",
        "#Function to convert the audio waveform into machine-readable 2d feature map (MFCCs)\n",
        "def extract_features(audio, sr=22050):\n",
        "    mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=38)\n",
        "\n",
        "    # Pad or crop to 98 time steps\n",
        "    if mfccs.shape[1] < 98:\n",
        "        pad_width = 98 - mfccs.shape[1]\n",
        "        mfccs = np.pad(mfccs, pad_width=((0, 0), (0, pad_width)), mode='constant')\n",
        "    else:\n",
        "        mfccs = mfccs[:, :98]\n",
        "\n",
        "    mfccs = np.expand_dims(mfccs, axis=-1)  # (38, 98, 1)\n",
        "    return mfccs\n",
        "\n",
        "#Function to predict stress based on the MFCC made by previous function\n",
        "def predict_audio(audio_file, model=audio_model):\n",
        "    try:\n",
        "        y, sr = librosa.load(audio_file, sr=22050)\n",
        "        features = extract_features(y, sr)\n",
        "        features = features.reshape(1, 38, 98, 1)  # Input shape for CNN\n",
        "\n",
        "        prediction = model.predict(features, verbose=0)[0][0]  # Binary classification\n",
        "\n",
        "        label = \"Stressed\" if prediction >= 0.5 else \"Not Stressed\"\n",
        "        confidence = prediction if prediction >= 0.5 else 1 - prediction\n",
        "\n",
        "        return label, confidence\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Audio Prediction Error: {str(e)}\")\n",
        "        return \"Error\", 0.0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9mfce-R2LkH"
      },
      "source": [
        "FUSION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "YRwYwJkb2M78"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "#Function to get the fused(weighted) average confidence of the prediction\n",
        "def get_stress_confidence(label, confidence):\n",
        "    if label.lower() == 'stressed':\n",
        "        return float(confidence)\n",
        "    else:\n",
        "        return 1.0 - float(confidence)\n",
        "\n",
        "def agreement_fusion(confidences):\n",
        "    M = len(confidences)\n",
        "    agree_scores = []\n",
        "    for i in range(M):\n",
        "        total_agree = 0\n",
        "        for j in range(M):\n",
        "            if i != j:\n",
        "                total_agree += (1 - abs(confidences[i] - confidences[j]))\n",
        "        agree_i = total_agree / (M - 1)\n",
        "        agree_scores.append(agree_i)\n",
        "    agree_scores = np.array(agree_scores)\n",
        "    fused = np.sum(agree_scores * confidences) / np.sum(agree_scores)\n",
        "    return fused"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "BdB7aGwaCQ1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 648
        },
        "outputId": "996bbea7-bd27-481c-9997-f64a938371c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://9cc700a847d376e5e6.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://9cc700a847d376e5e6.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "import gradio as gr\n",
        "def fused_stress_prediction(audio_file, photo_path,\n",
        "                            activity, companion, pressure, tired, energetic):\n",
        "\n",
        "    try:\n",
        "        print(\"üì• Inputs Received:\")\n",
        "        print(\"Audio:\", audio_file)\n",
        "        print(\"Image:\", photo_path)\n",
        "        print(\"Survey:\", activity, companion, pressure, tired, energetic)\n",
        "\n",
        "        # üîç Predict from individual modalities\n",
        "        label_audio, conf_audio = predict_audio(audio_file, audio_model)\n",
        "        label_facial, conf_facial = predict_facial(photo_path, facial_model)\n",
        "        label_survey, conf_survey = predict_survey(activity, companion, pressure, tired, energetic)\n",
        "\n",
        "        # üîó Fuse predictions into a single score\n",
        "        confidences = [\n",
        "            get_stress_confidence(label_audio, conf_audio),\n",
        "            get_stress_confidence(label_facial, conf_facial),\n",
        "            get_stress_confidence(label_survey, conf_survey)\n",
        "        ]\n",
        "        fused_score = round(agreement_fusion(confidences), 2)\n",
        "        print(f\"üîÅ Fused Stress Score: {fused_score:.2f}\")\n",
        "\n",
        "        # üß† Final label\n",
        "        label = \"üß† Stressed\" if fused_score >= 0.5 else \"üôÇ Not Stressed\"\n",
        "\n",
        "        return f\"{label} (Fused Score: {fused_score:.2f})\"\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"‚ö†Ô∏è Error: {str(e)}\"\n",
        "\n",
        "choices = [\n",
        "    [\"Working\", \"Studying\", \"Relaxing\", \"Exercising\", \"Other\"],\n",
        "    [\"Alone\", \"Friends\", \"Family\", \"Colleagues\", \"Strangers\"],\n",
        "    [\"Yes\", \"No\", \"Maybe\"],\n",
        "    [\"Yes\", \"No\"],\n",
        "    [\"Very Energetic\", \"Somewhat Energetic\", \"Neutral\", \"Tired\"]\n",
        "]\n",
        "# Gradio interface for increase ease of use\n",
        "iface = gr.Interface(\n",
        "    fn=fused_stress_prediction,\n",
        "    inputs=[\n",
        "        gr.Audio(type=\"filepath\", label=\"Voice\"),\n",
        "        gr.Image(type=\"filepath\", label=\"Photo\"),\n",
        "        gr.Radio(choices=choices[0], label=\"What activity are you currently engaged in?\"),\n",
        "        gr.Radio(choices=choices[1], label=\"Who are you currently with?\"),\n",
        "        gr.Radio(choices=choices[2], label=\"Do you feel under pressure right now?\"),\n",
        "        gr.Radio(choices=choices[3], label=\"Do you feel tired?\"),\n",
        "        gr.Radio(choices=choices[4], label=\"Do you feel energetic right now?\")\n",
        "    ],\n",
        "    outputs=\"text\"\n",
        ")\n",
        "\n",
        "iface.launch()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}